{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rbSTui5Qz0OZ",
    "outputId": "134a6f63-fb5f-4302-b1eb-bccaca0312a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting librosa\n",
      "  Using cached librosa-0.10.2.post1-py3-none-any.whl.metadata (8.6 kB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement tensorflow (from versions: none)\n",
      "ERROR: No matching distribution found for tensorflow\n"
     ]
    }
   ],
   "source": [
    "!pip install librosa tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xFAD91r21YbR"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'librosa'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlibrosa\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers, models\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'librosa'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "from tensorflow.keras import layers, models\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z1sNN5J81eN2"
   },
   "outputs": [],
   "source": [
    "# Define the CMKD (CNN + Transformer) model\n",
    "def create_cmkd_model(input_shape, num_classes):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # CNN part\n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
    "    x = layers.Flatten()(x)\n",
    "\n",
    "    # Transformer part (Encoder Layer)\n",
    "    transformer_input = layers.Reshape((input_shape[0], -1))(inputs)\n",
    "    transformer = layers.MultiHeadAttention(num_heads=4, key_dim=64)(transformer_input, transformer_input)\n",
    "    transformer = layers.GlobalAveragePooling1D()(transformer)\n",
    "\n",
    "    # Knowledge Distillation (fusion of CNN and Transformer features)\n",
    "    merged = layers.Concatenate()([x, transformer])\n",
    "\n",
    "    # Fully connected layers\n",
    "    dense = layers.Dense(128, activation='relu')(merged)\n",
    "    outputs = layers.Dense(num_classes, activation='softmax')(dense)\n",
    "\n",
    "    model = models.Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7JUIaVD51brX"
   },
   "outputs": [],
   "source": [
    "# Load audio files and extract features\n",
    "def load_audio_files(directory, max_length=128):\n",
    "    labels = []\n",
    "    features = []\n",
    "\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".wav\"):\n",
    "            print(\"Processing file:\", filename)\n",
    "            # Load the audio file\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            audio, sr = librosa.load(file_path, sr=None)\n",
    "\n",
    "            # Extract Mel-spectrogram\n",
    "            mel_spec = librosa.feature.melspectrogram(y=audio, sr=sr, n_mels=128)\n",
    "            mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "            # Normalize the spectrogram\n",
    "            mel_spec_db = (mel_spec_db - np.min(mel_spec_db)) / (np.max(mel_spec_db) - np.min(mel_spec_db))\n",
    "\n",
    "            # Truncate or pad the spectrogram\n",
    "            if mel_spec_db.shape[1] > max_length:\n",
    "                mel_spec_db = mel_spec_db[:, :max_length]  # Truncate to max_length\n",
    "            elif mel_spec_db.shape[1] < max_length:\n",
    "                padding = np.zeros((128, max_length - mel_spec_db.shape[1]))  # Padding with zeros\n",
    "                mel_spec_db = np.hstack((mel_spec_db, padding))\n",
    "\n",
    "            # Get label (character) from the filename (e.g., 0.wav -> '0', a.wav -> 'a')\n",
    "            label = filename.split('.')[0]\n",
    "            labels.append(label)\n",
    "            features.append(mel_spec_db)\n",
    "\n",
    "    return np.array(features), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "DpTXR8mu1gI3",
    "outputId": "e1b4450b-f7ca-463e-a7a9-0f9f5f3c82f5"
   },
   "outputs": [],
   "source": [
    "# Load training data\n",
    "train_data_dir = '/content/drive/MyDrive/Project I/training_data/'\n",
    "X_train, y_train = load_audio_files(train_data_dir)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "\n",
    "# Create the model\n",
    "input_shape = (128, 128, 1)  # Adjust based on your spectrogram dimensions\n",
    "num_classes = len(np.unique(y_train_encoded))  # Number of unique classes\n",
    "cmkd_model = create_cmkd_model(input_shape, num_classes)\n",
    "\n",
    "# Compile the model\n",
    "cmkd_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print model summary\n",
    "cmkd_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oDRWUPPz9dv1",
    "outputId": "380e29f5-8970-450a-9360-083a7c0c7082"
   },
   "outputs": [],
   "source": [
    "# Reshape X_train to include the channel dimension if not already done\n",
    "if len(X_train.shape) == 3:  # If missing the channel dimension\n",
    "    X_train = X_train.reshape(X_train.shape[0], 128, 128, 1)\n",
    "\n",
    "# Set batch size and calculate steps per epoch\n",
    "batch_size = 32\n",
    "steps_per_epoch = len(X_train) // batch_size  # Calculate steps per epoch\n",
    "\n",
    "# Train the model\n",
    "history = cmkd_model.fit(X_train, y_train_encoded, epochs=150, batch_size=batch_size, steps_per_epoch=steps_per_epoch, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "zcjHzeUtA_qF"
   },
   "outputs": [],
   "source": [
    "# Function to load the audio file\n",
    "def load_audio(file_path):\n",
    "    audio, sample_rate = librosa.load(file_path, sr=None)\n",
    "    return audio, sample_rate\n",
    "\n",
    "# Detect regions of continuous sound and pauses between them\n",
    "def detect_sound_regions(audio, sample_rate, silence_threshold=0.01, min_pause_duration=0.8):\n",
    "    rms = librosa.feature.rms(y=audio)[0]\n",
    "    frame_duration = 512 / sample_rate\n",
    "    min_pause_frames = int(min_pause_duration / frame_duration)\n",
    "\n",
    "    sound_regions = []\n",
    "    is_silence = rms < silence_threshold\n",
    "\n",
    "    start = None\n",
    "    for i in range(len(is_silence)):\n",
    "        if not is_silence[i] and start is None:\n",
    "            start = i  # Sound started\n",
    "        elif is_silence[i] and start is not None:\n",
    "            if np.all(is_silence[i:i + min_pause_frames]):\n",
    "                sound_regions.append((start, i))\n",
    "                start = None\n",
    "\n",
    "    return sound_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "f6DIJTYyBBW0"
   },
   "outputs": [],
   "source": [
    "# Split audio by detected sound regions and save each region as a letter\n",
    "def split_and_save_letters(audio, sample_rate, sound_regions, output_folder):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    file_count = 1  # Counter for the letter file names\n",
    "\n",
    "    for region in sound_regions:\n",
    "        start_sample = region[0] * 512  # Convert RMS index to sample index\n",
    "        end_sample = region[1] * 512\n",
    "\n",
    "        letter_audio = audio[start_sample:end_sample]\n",
    "        segment_filename = os.path.join(output_folder, f'letter{file_count}.wav')\n",
    "\n",
    "        # Save the letter audio as a .wav file\n",
    "        sf.write(segment_filename, letter_audio, sample_rate)\n",
    "        file_count += 1\n",
    "\n",
    "    print(f\"Saved {file_count - 1} letter files to {output_folder}\")\n",
    "\n",
    "# Main process: load audio, detect sound regions, and split into letters\n",
    "def process_word_audio(input_file_path, output_folder):\n",
    "    audio, sample_rate = load_audio(input_file_path)\n",
    "    sound_regions = detect_sound_regions(audio, sample_rate)\n",
    "    split_and_save_letters(audio, sample_rate, sound_regions, output_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "bTUs_R15BF0t"
   },
   "outputs": [],
   "source": [
    "# Extract mel features from segmented files\n",
    "def extract_mel_features(file_path, max_length=128):\n",
    "    audio, sample_rate = librosa.load(file_path)\n",
    "    mel_spec = librosa.feature.melspectrogram(y=audio, sr=sample_rate, n_mels=128)\n",
    "    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "\n",
    "    # Normalize the spectrogram\n",
    "    mel_spec_db = (mel_spec_db - np.min(mel_spec_db)) / (np.max(mel_spec_db) - np.min(mel_spec_db))\n",
    "\n",
    "    # Truncate or pad the spectrogram\n",
    "    if mel_spec_db.shape[1] > max_length:\n",
    "        mel_spec_db = mel_spec_db[:, :max_length]\n",
    "    elif mel_spec_db.shape[1] < max_length:\n",
    "        padding = np.zeros((128, max_length - mel_spec_db.shape[1]))\n",
    "        mel_spec_db = np.hstack((mel_spec_db, padding))\n",
    "\n",
    "    return mel_spec_db\n",
    "\n",
    "# Extract features from all segmented files\n",
    "def extract_features_from_segmented(output_folder):\n",
    "    features = []\n",
    "    file_names = []\n",
    "\n",
    "    for file_name in os.listdir(output_folder):\n",
    "        if file_name.endswith('.wav'):\n",
    "            file_path = os.path.join(output_folder, file_name)\n",
    "            mel_features = extract_mel_features(file_path)\n",
    "            features.append(mel_features)\n",
    "            file_names.append(file_name)\n",
    "\n",
    "    return np.array(features), file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VDrOOfiHBEVt",
    "outputId": "6e8b7590-18f7-427a-9fc9-4940420d2131"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'librosa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m OUTPUT_FOLDER \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./letters/\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Folder to store the individual letter files\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Run the process for the given audio file\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[43mprocess_word_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mINPUT_FILE_PATH\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOUTPUT_FOLDER\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 23\u001b[0m, in \u001b[0;36mprocess_word_audio\u001b[1;34m(input_file_path, output_folder)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_word_audio\u001b[39m(input_file_path, output_folder):\n\u001b[1;32m---> 23\u001b[0m     audio, sample_rate \u001b[38;5;241m=\u001b[39m \u001b[43mload_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     sound_regions \u001b[38;5;241m=\u001b[39m detect_sound_regions(audio, sample_rate)\n\u001b[0;32m     25\u001b[0m     split_and_save_letters(audio, sample_rate, sound_regions, output_folder)\n",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m, in \u001b[0;36mload_audio\u001b[1;34m(file_path)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_audio\u001b[39m(file_path):\n\u001b[1;32m----> 3\u001b[0m     audio, sample_rate \u001b[38;5;241m=\u001b[39m \u001b[43mlibrosa\u001b[49m\u001b[38;5;241m.\u001b[39mload(file_path, sr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m audio, sample_rate\n",
      "\u001b[1;31mNameError\u001b[0m: name 'librosa' is not defined"
     ]
    }
   ],
   "source": [
    "# Define paths for audio processing\n",
    "INPUT_FILE_PATH = '/content/drive/MyDrive/Project I/testing_data/HELLO.wav'  # Path to the input audio file\n",
    "OUTPUT_FOLDER = './letters/'  # Folder to store the individual letter files\n",
    "\n",
    "# Run the process for the given audio file\n",
    "process_word_audio(INPUT_FILE_PATH, OUTPUT_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ErbAKfWCBH2D",
    "outputId": "98fb9bc7-e5a9-4286-c3f4-55e5620940d7"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: './letters/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Call the function to extract features\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m X_segmented, file_names_segmented \u001b[38;5;241m=\u001b[39m \u001b[43mextract_features_from_segmented\u001b[49m\u001b[43m(\u001b[49m\u001b[43mOUTPUT_FOLDER\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Reshape for model input\u001b[39;00m\n\u001b[0;32m      5\u001b[0m X_segmented \u001b[38;5;241m=\u001b[39m X_segmented\u001b[38;5;241m.\u001b[39mreshape(X_segmented\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 24\u001b[0m, in \u001b[0;36mextract_features_from_segmented\u001b[1;34m(output_folder)\u001b[0m\n\u001b[0;32m     21\u001b[0m features \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     22\u001b[0m file_names \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m---> 24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_folder\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_name\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.wav\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m     26\u001b[0m         file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_folder, file_name)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: './letters/'"
     ]
    }
   ],
   "source": [
    "# Call the function to extract features\n",
    "X_segmented, file_names_segmented = extract_features_from_segmented(OUTPUT_FOLDER)\n",
    "\n",
    "# Reshape for model input\n",
    "X_segmented = X_segmented.reshape(X_segmented.shape[0], 128, 128, 1)\n",
    "\n",
    "# Predict characters from segmented features\n",
    "predictions = cmkd_model.predict(X_segmented)\n",
    "\n",
    "# Convert predictions to classes\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Map predictions back to original characters\n",
    "decoded_labels = label_encoder.inverse_transform(predicted_classes)\n",
    "\n",
    "# Print the results\n",
    "for file_name, predicted_label in zip(file_names_segmented, decoded_labels):\n",
    "    print(f\"{file_name} predicted class: {predicted_label}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
