{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the Morse code audio files\n",
    "DATA_PATH = './morse_code_train_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract MFCC features from an audio file\n",
    "def extract_features(file_path):\n",
    "    audio, sample_rate = librosa.load(file_path)\n",
    "    # Extract 40 MFCC features\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "    # Take the mean of the MFCCs over time to reduce the feature dimensionality\n",
    "    mfccs_mean = np.mean(mfccs.T, axis=0)\n",
    "    return mfccs_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset and extract features\n",
    "def load_data():\n",
    "    features = []  # To store MFCC features\n",
    "    labels = []    # To store the corresponding labels (letters/numbers)\n",
    "    \n",
    "    # Loop through all files in the data directory\n",
    "    for file_name in os.listdir(DATA_PATH):\n",
    "        if file_name.endswith('.wav'):\n",
    "            # Extract the label from the file name (e.g., 'A.wav' -> label 'A')\n",
    "            label = file_name[0]  # Assuming the first character of the filename is the label\n",
    "            file_path = os.path.join(DATA_PATH, file_name)\n",
    "            \n",
    "            # Extract features from the audio file\n",
    "            mfccs = extract_features(file_path)\n",
    "            \n",
    "            # Append the features and label to their respective lists\n",
    "            features.append(mfccs)\n",
    "            labels.append(label)\n",
    "    \n",
    "    return np.array(features), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features shape: (36, 40), Labels shape: (36,)\n"
     ]
    }
   ],
   "source": [
    "# Call the load_data function to get the features and labels\n",
    "X, y = load_data()\n",
    "\n",
    "# Print shapes of X and y to confirm\n",
    "print(f\"Features shape: {X.shape}, Labels shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features shape: (36, 40), Training Labels shape: (36,)\n",
      "Testing Features shape: (9, 40), Testing Labels shape: (9,)\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Path to the Morse code audio files\n",
    "DATA_PATH = './morse_code_train_data/'\n",
    "\n",
    "# Function to extract MFCC features from an audio file\n",
    "def extract_features(file_path):\n",
    "    audio, sample_rate = librosa.load(file_path)\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "    return np.mean(mfccs.T, axis=0)\n",
    "\n",
    "# Load the dataset and extract features\n",
    "def load_data():\n",
    "    features = []  # To store MFCC features\n",
    "    labels = []    # To store the corresponding labels (letters/numbers)\n",
    "    \n",
    "    # Loop through all files in the data directory\n",
    "    for file_name in os.listdir(DATA_PATH):\n",
    "        if file_name.endswith('.wav'):\n",
    "            label = file_name[0]  # Assuming the first character of the filename is the label\n",
    "            file_path = os.path.join(DATA_PATH, file_name)\n",
    "            mfccs = extract_features(file_path)\n",
    "            features.append(mfccs)\n",
    "            labels.append(label)\n",
    "    \n",
    "    return np.array(features), np.array(labels)\n",
    "\n",
    "# Load the full dataset\n",
    "X, y = load_data()\n",
    "\n",
    "# Manually specify which files to include in the testing set\n",
    "# Example: Let's say we want to test on these letters\n",
    "testing_files = ['A.wav', 'B.wav', 'C.wav', 'Z.wav', 'Y.wav', 'X.wav', '1.wav', '0.wav', 'H.wav']  # Add any letters you want to test\n",
    "\n",
    "# Initialize lists for training and testing data\n",
    "X_train, y_train = [], []\n",
    "X_test, y_test = [], []\n",
    "\n",
    "# Split the data\n",
    "for file_name in os.listdir(DATA_PATH):\n",
    "    if file_name.endswith('.wav'):\n",
    "        # If the file is in the testing set, add to test data\n",
    "        if file_name in testing_files:\n",
    "            index = np.where(y == file_name[0])[0][0]  # Get index of the label in original labels\n",
    "            X_test.append(X[index])  # Add corresponding features to test set\n",
    "            y_test.append(y[index])  # Add corresponding label to test set\n",
    "        \n",
    "        index = np.where(y == file_name[0])[0][0]  # Get index of the label in original labels\n",
    "        X_train.append(X[index])  # Add corresponding features to train set\n",
    "        y_train.append(y[index])  # Add corresponding label to train set\n",
    "\n",
    "# Convert lists back to numpy arrays\n",
    "X_train = np.array(X_train)\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "# Print shapes to confirm split\n",
    "print(f\"Training Features shape: {X_train.shape}, Training Labels shape: {y_train.shape}\")\n",
    "print(f\"Testing Features shape: {X_test.shape}, Testing Labels shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model and testing it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 100.00%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         1\n",
      "           1       1.00      1.00      1.00         1\n",
      "           A       1.00      1.00      1.00         1\n",
      "           B       1.00      1.00      1.00         1\n",
      "           C       1.00      1.00      1.00         1\n",
      "           H       1.00      1.00      1.00         1\n",
      "           X       1.00      1.00      1.00         1\n",
      "           Y       1.00      1.00      1.00         1\n",
      "           Z       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         9\n",
      "   macro avg       1.00      1.00      1.00         9\n",
      "weighted avg       1.00      1.00      1.00         9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Initialize the SVM classifier\n",
    "classifier = SVC(kernel='linear')\n",
    "\n",
    "# Train the classifier on the training data\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Get unique labels from the test data for classification report\n",
    "target_names = np.unique(y_test)\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(y_test, y_pred, target_names=target_names)\n",
    "\n",
    "# Print results\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 100.00%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         1\n",
      "           1       1.00      1.00      1.00         1\n",
      "           A       1.00      1.00      1.00         1\n",
      "           B       1.00      1.00      1.00         1\n",
      "           C       1.00      1.00      1.00         1\n",
      "           H       1.00      1.00      1.00         1\n",
      "           X       1.00      1.00      1.00         1\n",
      "           Y       1.00      1.00      1.00         1\n",
      "           Z       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         9\n",
      "   macro avg       1.00      1.00      1.00         9\n",
      "weighted avg       1.00      1.00      1.00         9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize the Random Forest classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 22.22%\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         1\n",
      "           1       0.50      1.00      0.67         1\n",
      "           2       0.00      0.00      0.00         0\n",
      "           4       0.00      0.00      0.00         0\n",
      "           6       0.00      0.00      0.00         0\n",
      "           8       0.00      0.00      0.00         0\n",
      "           A       0.00      0.00      0.00         1\n",
      "           B       0.00      0.00      0.00         1\n",
      "           C       0.00      0.00      0.00         1\n",
      "           H       0.00      0.00      0.00         1\n",
      "           X       0.00      0.00      0.00         1\n",
      "           Y       0.00      0.00      0.00         1\n",
      "           Z       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.22         9\n",
      "   macro avg       0.12      0.15      0.13         9\n",
      "weighted avg       0.17      0.22      0.19         9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hanon Systems\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Hanon Systems\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Hanon Systems\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Hanon Systems\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Hanon Systems\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Hanon Systems\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Initialize the KNN classifier\n",
    "knn_classifier = KNeighborsClassifier(n_neighbors=5)  # You can adjust n_neighbors based on your needs\n",
    "\n",
    "# Train the classifier on the training data\n",
    "knn_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = knn_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Generate classification report\n",
    "report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Print results\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Model Accuracy: 100.00%\n",
      "Decision Tree Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         1\n",
      "           1       1.00      1.00      1.00         1\n",
      "           A       1.00      1.00      1.00         1\n",
      "           B       1.00      1.00      1.00         1\n",
      "           C       1.00      1.00      1.00         1\n",
      "           H       1.00      1.00      1.00         1\n",
      "           X       1.00      1.00      1.00         1\n",
      "           Y       1.00      1.00      1.00         1\n",
      "           Z       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         9\n",
      "   macro avg       1.00      1.00      1.00         9\n",
      "weighted avg       1.00      1.00      1.00         9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Initialize the Decision Tree classifier\n",
    "dt_classifier = DecisionTreeClassifier()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "dt_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_dt = dt_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_dt = accuracy_score(y_test, y_pred_dt)\n",
    "\n",
    "# Generate classification report\n",
    "report_dt = classification_report(y_test, y_pred_dt)\n",
    "\n",
    "# Print results\n",
    "print(f\"Decision Tree Model Accuracy: {accuracy_dt * 100:.2f}%\")\n",
    "print(\"Decision Tree Classification Report:\")\n",
    "print(report_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model Accuracy: 88.89%\n",
      "Logistic Regression Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         1\n",
      "           1       1.00      1.00      1.00         1\n",
      "           A       0.00      0.00      0.00         1\n",
      "           B       1.00      1.00      1.00         1\n",
      "           C       1.00      1.00      1.00         1\n",
      "           H       1.00      1.00      1.00         1\n",
      "           N       0.00      0.00      0.00         0\n",
      "           X       1.00      1.00      1.00         1\n",
      "           Y       1.00      1.00      1.00         1\n",
      "           Z       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.89         9\n",
      "   macro avg       0.80      0.80      0.80         9\n",
      "weighted avg       0.89      0.89      0.89         9\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Hanon Systems\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Hanon Systems\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Hanon Systems\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Hanon Systems\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Hanon Systems\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\Hanon Systems\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Initialize the Logistic Regression classifier\n",
    "lr_classifier = LogisticRegression(max_iter=1000)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "lr_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_logistic = lr_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_logistic = accuracy_score(y_test, y_pred_logistic)\n",
    "\n",
    "# Generate classification report\n",
    "report_logistic = classification_report(y_test, y_pred_logistic)\n",
    "\n",
    "# Print results\n",
    "print(f\"Logistic Regression Model Accuracy: {accuracy_logistic * 100:.2f}%\")\n",
    "print(\"Logistic Regression Classification Report:\")\n",
    "print(report_logistic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Boosting Model Accuracy: 100.00%\n",
      "Gradient Boosting Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         1\n",
      "           1       1.00      1.00      1.00         1\n",
      "           A       1.00      1.00      1.00         1\n",
      "           B       1.00      1.00      1.00         1\n",
      "           C       1.00      1.00      1.00         1\n",
      "           H       1.00      1.00      1.00         1\n",
      "           X       1.00      1.00      1.00         1\n",
      "           Y       1.00      1.00      1.00         1\n",
      "           Z       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         9\n",
      "   macro avg       1.00      1.00      1.00         9\n",
      "weighted avg       1.00      1.00      1.00         9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Initialize the Gradient Boosting classifier\n",
    "gb_classifier = GradientBoostingClassifier()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "gb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_gb = gb_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
    "\n",
    "# Generate classification report\n",
    "report_gb = classification_report(y_test, y_pred_gb)\n",
    "\n",
    "# Print results\n",
    "print(f\"Gradient Boosting Model Accuracy: {accuracy_gb * 100:.2f}%\")\n",
    "print(\"Gradient Boosting Classification Report:\")\n",
    "print(report_gb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Model Accuracy: 100.00%\n",
      "Naive Bayes Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         1\n",
      "           1       1.00      1.00      1.00         1\n",
      "           A       1.00      1.00      1.00         1\n",
      "           B       1.00      1.00      1.00         1\n",
      "           C       1.00      1.00      1.00         1\n",
      "           H       1.00      1.00      1.00         1\n",
      "           X       1.00      1.00      1.00         1\n",
      "           Y       1.00      1.00      1.00         1\n",
      "           Z       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         9\n",
      "   macro avg       1.00      1.00      1.00         9\n",
      "weighted avg       1.00      1.00      1.00         9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Initialize the Naive Bayes classifier\n",
    "nb_classifier = GaussianNB()\n",
    "\n",
    "# Train the classifier on the training data\n",
    "nb_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_nb = nb_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy_nb = accuracy_score(y_test, y_pred_nb)\n",
    "\n",
    "# Generate classification report\n",
    "report_nb = classification_report(y_test, y_pred_nb)\n",
    "\n",
    "# Print results\n",
    "print(f\"Naive Bayes Model Accuracy: {accuracy_nb * 100:.2f}%\")\n",
    "print(\"Naive Bayes Classification Report:\")\n",
    "print(report_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dumping the Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./models/naive_bayes_model.pkl']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained SVM model\n",
    "joblib.dump(classifier, './models/svm_model.pkl')\n",
    "\n",
    "# Save the trained Random Forest model\n",
    "joblib.dump(rf_classifier, './models/random_forest_model.pkl')\n",
    "\n",
    "# Save the trained KNN model\n",
    "joblib.dump(knn_classifier, './models/knn_model.pkl')\n",
    "\n",
    "# Save the trained Decision Tree model\n",
    "joblib.dump(dt_classifier, './models/decision_tree_model.pkl')\n",
    "\n",
    "# Save the trained Logistic Regression model\n",
    "joblib.dump(lr_classifier, './models/logistic_regression_model.pkl')\n",
    "\n",
    "# Save the trained Gradient Boosting model\n",
    "joblib.dump(gb_classifier, './models/gradient_boosting_model.pkl')\n",
    "\n",
    "# Save the trained Naive Bayes model\n",
    "joblib.dump(nb_classifier, './models/naive_bayes_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Data on Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 5 letter files to ./letters/\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "\n",
    "# Define paths\n",
    "INPUT_FILE_PATH = 'morse_code_testing/HELLO.wav'  # Path to the input audio file\n",
    "OUTPUT_FOLDER = './letters/'  # Folder to store the individual letter files\n",
    "\n",
    "# Function to load the audio file\n",
    "def load_audio(file_path):\n",
    "    audio, sample_rate = librosa.load(file_path)\n",
    "    return audio, sample_rate\n",
    "\n",
    "# Detect regions of continuous sound and pauses between them\n",
    "def detect_sound_regions(audio, sample_rate, silence_threshold=0.01, min_pause_duration=1.2):\n",
    "    rms = librosa.feature.rms(y=audio)[0]  # Compute RMS (energy) for detecting silence\n",
    "    frame_duration = 512 / sample_rate  # Each RMS frame represents this many seconds\n",
    "    min_pause_frames = int(min_pause_duration / frame_duration)  # Convert pause duration to frames\n",
    "\n",
    "    sound_regions = []\n",
    "    is_silence = rms < silence_threshold\n",
    "\n",
    "    # Find indices where sound starts and ends, and where long pauses occur\n",
    "    start = None\n",
    "    for i in range(len(is_silence)):\n",
    "        if not is_silence[i] and start is None:\n",
    "            start = i  # Sound started\n",
    "        elif is_silence[i] and start is not None:\n",
    "            # If a long enough pause is detected, mark the end of a letter\n",
    "            if np.all(is_silence[i:i + min_pause_frames]):\n",
    "                sound_regions.append((start, i))\n",
    "                start = None\n",
    "\n",
    "    return sound_regions\n",
    "\n",
    "# Split audio by detected sound regions and save each region as a letter\n",
    "def split_and_save_letters(audio, sample_rate, sound_regions, output_folder):\n",
    "    # Create the output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    file_count = 1  # Counter for the letter file names\n",
    "\n",
    "    for region in sound_regions:\n",
    "        start_sample = region[0] * 512  # Convert RMS index to sample index\n",
    "        end_sample = region[1] * 512\n",
    "        \n",
    "        letter_audio = audio[start_sample:end_sample]\n",
    "        segment_filename = os.path.join(output_folder, f'letter{file_count}.wav')\n",
    "        \n",
    "        # Save the letter audio as a .wav file\n",
    "        sf.write(segment_filename, letter_audio, sample_rate)\n",
    "        file_count += 1\n",
    "\n",
    "    print(f\"Saved {file_count - 1} letter files to {output_folder}\")\n",
    "\n",
    "# Main process: load audio, detect sound regions, and split into letters\n",
    "def process_word_audio(input_file_path, output_folder):\n",
    "    # Load the audio file\n",
    "    audio, sample_rate = load_audio(input_file_path)\n",
    "    \n",
    "    # Detect continuous sound regions (letters)\n",
    "    sound_regions = detect_sound_regions(audio, sample_rate)\n",
    "    \n",
    "    # Split the audio into letters and save them\n",
    "    split_and_save_letters(audio, sample_rate, sound_regions, output_folder)\n",
    "\n",
    "# Run the process for the given audio file\n",
    "process_word_audio(INPUT_FILE_PATH, OUTPUT_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted features from 5 segmented audio files.\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Function to extract MFCC features from an audio file\n",
    "def extract_features(file_path):\n",
    "    audio, sample_rate = librosa.load(file_path)\n",
    "    mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "    return np.mean(mfccs.T, axis=0)\n",
    "\n",
    "# Path to the segmented letter audio files\n",
    "SEGMENTED_DATA_PATH = 'letters/'  # Modify as per your file structure\n",
    "\n",
    "# Extract features from each segmented file\n",
    "def extract_features_from_segmented():\n",
    "    features = []\n",
    "    file_names = []\n",
    "\n",
    "    for file_name in os.listdir(SEGMENTED_DATA_PATH):\n",
    "        if file_name.endswith('.wav'):\n",
    "            file_path = os.path.join(SEGMENTED_DATA_PATH, file_name)\n",
    "            mfccs = extract_features(file_path)\n",
    "            features.append(mfccs)\n",
    "            file_names.append(file_name)  # Keep track of file names for reference\n",
    "\n",
    "    return np.array(features), file_names\n",
    "\n",
    "# Extract features from the segmented files\n",
    "X_segmented, file_names_segmented = extract_features_from_segmented()\n",
    "\n",
    "print(f\"Extracted features from {len(X_segmented)} segmented audio files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib  # Or you can use pickle\n",
    "\n",
    "# Load the pre-trained models\n",
    "svm_model = joblib.load('./models/svm_model.pkl')\n",
    "rf_model = joblib.load('./models/random_forest_model.pkl')\n",
    "knn_model = joblib.load('./models/knn_model.pkl')\n",
    "dt_model = joblib.load('./models/decision_tree_model.pkl')\n",
    "gb_model = joblib.load('./models/gradient_boosting_model.pkl')\n",
    "nb_model = joblib.load('./models/naive_bayes_model.pkl')\n",
    "\n",
    "# Optionally, store them in a dictionary for easier access\n",
    "models = {\n",
    "    'SVM': svm_model,\n",
    "    'Random Forest': rf_model,\n",
    "    'KNN': knn_model,\n",
    "    'Decision Tree': dt_model,\n",
    "    'Gradient Boosting': gb_model,\n",
    "    'Naive Bayes': nb_model\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Predictions: ['H' 'H' '4' '4' '0']\n",
      "Random Forest Predictions: ['H' '4' 'U' 'U' 'M']\n",
      "KNN Predictions: ['4' '4' '4' '4' '0']\n",
      "Decision Tree Predictions: ['S' '4' 'B' 'B' 'M']\n",
      "Gradient Boosting Predictions: ['5' '5' '5' '5' '0']\n",
      "Naive Bayes Predictions: ['H' 'H' '4' '4' '0']\n",
      "SVM predicted word: HH440\n",
      "Random Forest predicted word: H4UUM\n",
      "KNN predicted word: 44440\n",
      "Decision Tree predicted word: S4BBM\n",
      "Gradient Boosting predicted word: 55550\n",
      "Naive Bayes predicted word: HH440\n"
     ]
    }
   ],
   "source": [
    "# Predict characters using each model\n",
    "predictions = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    y_pred = model.predict(X_segmented)\n",
    "    predictions[model_name] = y_pred\n",
    "    print(f\"{model_name} Predictions: {y_pred}\")\n",
    "\n",
    "# Combine predictions from different models\n",
    "for model_name, pred in predictions.items():\n",
    "    predicted_word = ''.join(pred)\n",
    "    print(f\"{model_name} predicted word: {predicted_word}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
